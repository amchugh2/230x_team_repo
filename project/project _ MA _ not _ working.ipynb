{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85a6c225-8666-433e-825d-04860658289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import zstandard as zstd\n",
    "import io\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0280e0-f5f7-476a-9416-6b03673ab310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1ea7205-21de-4963-b3b1-8c59ec9adc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(directory, ticker):\n",
    "    \"\"\"\n",
    "    Convert input csv to dataframes and prepare to process through trading signal.\n",
    "\n",
    "    Params:\n",
    "    directory (String): Name of directory that contains list of csv files.\n",
    "\n",
    "    Returns:\n",
    "    return (list[DataFrame]): List of pandas DataFrames, one for each trading day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Directory containing the CSV files\n",
    "    \n",
    "    ret = []\n",
    "    i = 1\n",
    "    # Loop through all files in the directory and convert .zst csv to df\n",
    "    for filename in os.listdir(directory):\n",
    "        \n",
    "        if filename.endswith('.zst'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            try:\n",
    "                # Open the compressed file in binary mode\n",
    "                with open(filepath, 'rb') as compressed_file:\n",
    "                    # Initialize the decompressor\n",
    "                    dctx = zstd.ZstdDecompressor()\n",
    "                    \n",
    "                    # Decompress the file into an in-memory buffer\n",
    "                    with dctx.stream_reader(compressed_file) as decompressed_stream:\n",
    "                        text_stream = io.TextIOWrapper(decompressed_stream, encoding='utf-8')\n",
    "                        print(f\"Processing {filename}\\n\")\n",
    "                        # Read the decompressed data into a pandas DataFrame\n",
    "                        df = pd.read_csv(text_stream, parse_dates=['ts_recv', 'ts_event'])\n",
    "                        print(str(i) + ' read file')\n",
    "                        df.to_pickle(os.path.join(directory,\"day_\" + str(i) + \".pkl\"))\n",
    "                        print(str(i) + ' saved file as pickle')\n",
    "                        i +=1\n",
    "                        print(f\"Processed {filename}\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}\\n\")\n",
    "                print(e)\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8024aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files_uncompressed(directory, ticker):\n",
    "    \"\"\"\n",
    "    Convert input csv to dataframes and prepare to process through trading signal.\n",
    "\n",
    "    Params:\n",
    "    directory (String): Name of directory that contains list of csv files.\n",
    "\n",
    "    Returns:\n",
    "    return (list[DataFrame]): List of pandas DataFrames, one for each trading day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Directory containing the CSV files\n",
    "    \n",
    "    ret = []\n",
    "    i = 1\n",
    "    # Loop through all files in the directory and convert .zst csv to df\n",
    "    for filename in os.listdir(directory):\n",
    "        \n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            try:\n",
    "                print(f\"Processing {filename}\\n\")\n",
    "                # Read the decompressed data into a pandas DataFrame\n",
    "                df = pd.read_csv(filepath, parse_dates=['ts_recv', 'ts_event'])\n",
    "                print(str(i) + ' read file')\n",
    "                df.to_pickle(os.path.join(directory,\"day_\" + str(i) + \".pkl\"))\n",
    "                print(str(i) + ' saved file as pickle')\n",
    "                i +=1\n",
    "                print(f\"Processed {filename}\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}\\n\")\n",
    "                print(e)\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c71bbc-ff38-40b2-bdac-a80f71fabc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded day_19.pkl\n",
      "Loaded day_18.pkl\n",
      "Loaded day_20.pkl\n",
      "Loaded day_21.pkl\n",
      "Loaded day_22.pkl\n",
      "Loaded day_9.pkl\n",
      "Loaded day_8.pkl\n",
      "Loaded day_6.pkl\n",
      "Loaded day_7.pkl\n",
      "Loaded day_5.pkl\n",
      "Loaded day_4.pkl\n",
      "Loaded day_1.pkl\n",
      "Loaded day_3.pkl\n",
      "Loaded day_2.pkl\n",
      "Loaded day_13.pkl\n",
      "Loaded day_12.pkl\n",
      "Loaded day_10.pkl\n",
      "Loaded day_11.pkl\n",
      "Loaded day_15.pkl\n",
      "Loaded day_14.pkl\n",
      "Loaded day_16.pkl\n",
      "Loaded day_17.pkl\n"
     ]
    }
   ],
   "source": [
    "directory = 'equity-pkl'\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.pkl'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            dfs.append(data)\n",
    "            print(f\"Loaded {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc13649-fbaa-450d-9b86-441795d8f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_signal(df, short_window, long_window, b):\n",
    "    \"\"\"\n",
    "    Generate trading signals based on moving average cross-over strategy.\n",
    "\n",
    "    Params:\n",
    "    prices (Series): asset prices\n",
    "    short_window (int): Window size for the short-term moving average\n",
    "    long_window (int): Window size for the long-term moving average\n",
    "    b (int): Bandwidth parameter that determines the buy/sell thresholds\n",
    "\n",
    "    Return:\n",
    "    return (Series): Series with trading signals (+1 for buy, -1 for sell, 0 for hold)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create midprice column (average of bid and ask prices)\n",
    "    mid_price = (df['bid_px_00'] + df['ask_px_00']) / 2\n",
    "    \n",
    "    # Calculate short-term and long-term moving averages\n",
    "    short_ma = mid_price.rolling(window=short_window).mean()\n",
    "    long_ma = mid_price.rolling(window=long_window).mean()\n",
    "    \n",
    "    # Define thresholds\n",
    "    upper_threshold = (1 + b) * long_ma\n",
    "    lower_threshold = (1 - b) * long_ma\n",
    "    \n",
    "    signal = pd.Series(0, index=df.index)\n",
    "\n",
    "    # Generate buy signals (+1 where short_ma > upper_threshold)\n",
    "    signal[short_ma > upper_threshold] = 1\n",
    "    \n",
    "    # Generate sell signals (-1 where short_ma < lower_threshold)\n",
    "    signal[short_ma < lower_threshold] = -1\n",
    "\n",
    "    #Set signal to 0 at the end of the day to ensure no overnight positions\n",
    "    signal[df.index.time > pd.to_datetime('19:55').time()] = 0\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "025ed679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def execute_trading_signal(bbo_df, signal, order_size=1_000_000, execution_delay='0ms'):\n",
    "    \"\"\"\n",
    "    Execute the trading signal based on the available order book depth and calculate actual P&L with execution delays.\n",
    "    \n",
    "    Params:\n",
    "    bbo_df (DataFrame): DataFrame containing the order book and signals for each day.\n",
    "    signal (Series): Series containing the generated trading signals.\n",
    "    order_size (float): Size of the order for each trade (default $1M).\n",
    "    execution_delay (str): The execution delay (e.g., '0ms', '100ms', '1s') to simulate latency before trade execution.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Updated DataFrame with calculated returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize position and return\n",
    "    position = 0\n",
    "    shares_held = 0\n",
    "    bbo_df['return'] = 0\n",
    "\n",
    "    # Process MPB-10 data to get 3-level deep order book\n",
    "    bids = bbo_df[['bid_px_00', 'bid_sz_00', 'bid_px_01', 'bid_sz_01', 'bid_px_02', 'bid_sz_02']].copy()\n",
    "    asks = bbo_df[['ask_px_00', 'ask_sz_00', 'ask_px_01', 'ask_sz_01', 'ask_px_02', 'ask_sz_02']].copy()\n",
    "\n",
    "    # Convert the execution delay to a pandas Timedelta for future lookups\n",
    "    delay_timedelta = pd.to_timedelta(execution_delay)\n",
    "\n",
    "    for i in signal.index:\n",
    "        current_signal = signal.loc[i]\n",
    "\n",
    "        # Calculate the future timestamp for delayed execution\n",
    "        delayed_timestamp = i + delay_timedelta\n",
    "\n",
    "        # Ensure the delayed timestamp is within the bounds of the data by finding the next available one\n",
    "        if delayed_timestamp not in bbo_df.index:\n",
    "            # Use `searchsorted` to find the index of the next available timestamp\n",
    "            pos = bbo_df.index.searchsorted(delayed_timestamp)\n",
    "            \n",
    "            # If no valid future timestamp is found, skip to the next iteration\n",
    "            if pos >= len(bbo_df.index):\n",
    "                print(f\"No valid future timestamp available for {i}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            delayed_timestamp = bbo_df.index[pos]\n",
    "\n",
    "        # Buy logic: when signal is +1 and we don't have an open position\n",
    "        if current_signal == 1 and shares_held == 0:\n",
    "            shares_bought = 0\n",
    "            investment = 0\n",
    "\n",
    "            # Buy up to 3 levels of the ask side until $1M is spent\n",
    "            for level in range(3):\n",
    "                ask_price = asks.loc[delayed_timestamp, f'ask_px_0{level}']\n",
    "                ask_size = asks.loc[delayed_timestamp, f'ask_sz_0{level}']\n",
    "                level_investment = min(order_size - investment, ask_price * ask_size)\n",
    "                \n",
    "                # Calculate number of shares to buy\n",
    "                level_shares = level_investment / ask_price\n",
    "                shares_bought += level_shares\n",
    "                investment += level_investment\n",
    "\n",
    "                if investment >= order_size:\n",
    "                    break  # Exit loop when $1M investment is reached\n",
    "\n",
    "            position += investment\n",
    "            shares_held += shares_bought\n",
    "\n",
    "            # Mark the negative cash flow from buying\n",
    "            bbo_df.loc[i, 'return'] = -investment\n",
    "\n",
    "        # Sell logic: when signal is -1 and we don't have an open position\n",
    "        elif current_signal == -1 and shares_held > 0:\n",
    "            shares_sold = 0\n",
    "            revenue = 0\n",
    "\n",
    "            # Sell up to 3 levels of the bid side, until $1M worth of shares are sold\n",
    "            for level in range(3):\n",
    "                bid_price = bids.loc[delayed_timestamp, f'bid_px_0{level}']\n",
    "                bid_size = bids.loc[delayed_timestamp, f'bid_sz_0{level}']\n",
    "                level_revenue = min(order_size - revenue, bid_price * bid_size)\n",
    "\n",
    "                level_shares = level_revenue / bid_price\n",
    "                shares_sold += level_shares\n",
    "                revenue += level_revenue\n",
    "\n",
    "                if revenue >= order_size:\n",
    "                    break\n",
    "\n",
    "            position -= revenue\n",
    "            shares_held -= shares_sold\n",
    "\n",
    "            # Mark the positive cash flow from selling\n",
    "            bbo_df.loc[i, 'return'] = revenue\n",
    "\n",
    "        # Exit logic: signal turns 0 and we have an open position\n",
    "        elif current_signal == 0 and shares_held != 0:\n",
    "            # Liquidate the position by settling the open trade\n",
    "            if shares_held > 0:\n",
    "                bid_price = bids.loc[delayed_timestamp, 'bid_px_00']\n",
    "                sell_qty = min(shares_held, bids.loc[delayed_timestamp, 'bid_sz_00'])\n",
    "                revenue = sell_qty * bid_price\n",
    "                position -= revenue\n",
    "                shares_held -= sell_qty\n",
    "\n",
    "                # Mark the positive cash flow from selling\n",
    "                bbo_df.loc[i, 'return'] = revenue\n",
    "\n",
    "            elif shares_held < 0:\n",
    "                ask_price = asks.loc[delayed_timestamp, 'ask_px_00']\n",
    "                buy_qty = min(abs(shares_held), asks.loc[delayed_timestamp, 'ask_sz_00'])\n",
    "                investment = buy_qty * ask_price\n",
    "                position += investment\n",
    "                shares_held += buy_qty\n",
    "\n",
    "                # Mark the negative cash flow from buying\n",
    "                bbo_df.loc[i, 'return'] = -investment\n",
    "\n",
    "    # Scale the cumulative return\n",
    "    bbo_df['cumulative_return'] = (bbo_df['return']).cumsum()\n",
    "    bbo_df['scaled_return'] = bbo_df['cumulative_return'] / 1_000_000\n",
    "\n",
    "    return bbo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f4327-5913-4ab1-b7e4-589562624cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing day_1.pkl\n",
      "Successfully generated trading signal. Sending to generate returns.\n",
      "No valid future timestamp available for 2024-08-23 19:54:55.900000+00:00. Skipping.\n",
      "No valid future timestamp available for 2024-08-23 19:54:55.900000+00:00. Skipping.\n",
      "Returns successfully generated for day_1.pkl \n",
      "\n",
      "Delay 0ms return: -209.39000000000397\n",
      "Delay 100ms return: -209.39000000000397\n",
      "Delay 1s return: -209.39000000000397\n",
      "\n",
      "Processing day_2.pkl\n",
      "Successfully generated trading signal. Sending to generate returns.\n",
      "No valid future timestamp available for 2024-08-30 19:54:59.900000+00:00. Skipping.\n",
      "No valid future timestamp available for 2024-08-30 19:54:59.900000+00:00. Skipping.\n",
      "Returns successfully generated for day_2.pkl \n",
      "\n",
      "Delay 0ms return: -915.0300000000439\n",
      "Delay 100ms return: -915.0300000000439\n",
      "Delay 1s return: -915.0300000000439\n",
      "\n",
      "Processing day_3.pkl\n",
      "Successfully generated trading signal. Sending to generate returns.\n",
      "No valid future timestamp available for 2024-08-22 19:54:59+00:00. Skipping.\n",
      "No valid future timestamp available for 2024-08-22 19:54:59+00:00. Skipping.\n",
      "Returns successfully generated for day_3.pkl \n",
      "\n",
      "Delay 0ms return: -376.76999999999316\n",
      "Delay 100ms return: -376.76999999999316\n",
      "Delay 1s return: -376.76999999999316\n",
      "\n",
      "Processing day_4.pkl\n",
      "Successfully generated trading signal. Sending to generate returns.\n",
      "No valid future timestamp available for 2024-08-28 19:54:59.900000+00:00. Skipping.\n",
      "No valid future timestamp available for 2024-08-28 19:54:59.900000+00:00. Skipping.\n",
      "Returns successfully generated for day_4.pkl \n",
      "\n",
      "Delay 0ms return: -651.5000000000437\n",
      "Delay 100ms return: -651.5000000000437\n",
      "Delay 1s return: -651.5000000000437\n",
      "\n",
      "Processing day_5.pkl\n",
      "Successfully generated trading signal. Sending to generate returns.\n",
      "No valid future timestamp available for 2024-08-20 19:54:59.700000+00:00. Skipping.\n",
      "No valid future timestamp available for 2024-08-20 19:54:59.700000+00:00. Skipping.\n",
      "Returns successfully generated for day_5.pkl \n",
      "\n",
      "Delay 0ms return: -324.1300000000083\n",
      "Delay 100ms return: -324.1300000000083\n",
      "Delay 1s return: -324.1300000000083\n",
      "\n",
      "Processing day_6.pkl\n",
      "Successfully generated trading signal. Sending to generate returns.\n",
      "No valid future timestamp available for 2024-08-27 19:54:59.800000+00:00. Skipping.\n"
     ]
    }
   ],
   "source": [
    "return_sig_1 = []\n",
    "return_sig_2 = []\n",
    "return_sig_3 = []\n",
    "\n",
    "directory = 'equity-pkl'\n",
    "\n",
    "def extract_day(filename):\n",
    "    return int(filename.split('_')[1].split('.')[0])\n",
    "\n",
    "# Get all .pkl files in the directory and sort by day number\n",
    "file_list = [f for f in os.listdir(directory) if f.endswith('.pkl')]\n",
    "file_list_sorted = sorted(file_list, key=extract_day)\n",
    "\n",
    "for filename in file_list_sorted:\n",
    "    print('Processing ' + filename)\n",
    "    \n",
    "    filepath = os.path.join(directory, filename)\n",
    "    \n",
    "    with open(filepath, 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "        \n",
    "        if 'ts_event' in list(df.columns):\n",
    "            df = df.set_index('ts_event')\n",
    "                    \n",
    "        bbo_df = df.between_time('13:40', '19:55', inclusive='left')\n",
    "        tkr_df = bbo_df[bbo_df['symbol'] == 'ANF'].resample('100ms').last().ffill()\n",
    "\n",
    "        signal_1 = moving_average_signal(tkr_df, 50, 500, 0.0015)  # No delay\n",
    "        signal_2 = moving_average_signal(tkr_df, 100, 1000, 0.0025) # 100ms\n",
    "        signal_3 = moving_average_signal(tkr_df, 500, 5000, 0.0035) # 1s\n",
    "\n",
    "        print('Successfully generated trading signal. Sending to generate returns.')\n",
    "        \n",
    "        # Execute signals with the corresponding delays\n",
    "        returns_0ms = execute_trading_signal(tkr_df, signal_1, execution_delay='0ms')\n",
    "        returns_100ms = execute_trading_signal(tkr_df, signal_2, execution_delay='10ms')\n",
    "        returns_1s = execute_trading_signal(tkr_df, signal_3, execution_delay='100ms')\n",
    "        \n",
    "        print('Returns successfully generated for ' + filename, '\\n')\n",
    "\n",
    "        # Extract the cumulative returns\n",
    "        ret_0ms = returns_0ms[\"cumulative_return\"].tail(1).values[0]\n",
    "        ret_100ms = returns_100ms[\"cumulative_return\"].tail(1).values[0]\n",
    "        ret_1s = returns_1s[\"cumulative_return\"].tail(1).values[0]\n",
    "\n",
    "        # Append to respective return lists\n",
    "        return_sig_1.append(ret_0ms)\n",
    "        return_sig_2.append(ret_100ms)\n",
    "        return_sig_3.append(ret_1s)\n",
    "\n",
    "        print(f'Delay 0ms return: {ret_0ms}')\n",
    "        print(f'Delay 100ms return: {ret_100ms}')\n",
    "        print(f'Delay 1s return: {ret_1s}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb1fe3-bb52-4662-a3bb-801d4c4cebd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67649f39-b6a2-4059-be63-ed8c5dee823c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4bfdc5-1052-49f9-8263-067523141e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b225ca-b51a-4762-aec6-219d67c8f6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f76b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061d077-ee58-4bd3-b3f5-2a42817c4f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08affd-39d8-451b-9bb2-491550c00790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ced212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31047294-4819-4760-9e18-ab33fc25f177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
