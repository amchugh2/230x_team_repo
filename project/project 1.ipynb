{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85a6c225-8666-433e-825d-04860658289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import zstandard as zstd\n",
    "import io\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0280e0-f5f7-476a-9416-6b03673ab310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ea7205-21de-4963-b3b1-8c59ec9adc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(directory, ticker):\n",
    "    \"\"\"\n",
    "    Convert input csv to dataframes and prepare to process through trading signal.\n",
    "\n",
    "    Params:\n",
    "    directory (String): Name of directory that contains list of csv files.\n",
    "\n",
    "    Returns:\n",
    "    return (list[DataFrame]): List of pandas DataFrames, one for each trading day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Directory containing the CSV files\n",
    "    \n",
    "    ret = []\n",
    "    \n",
    "    # Loop through all files in the directory and convert .zst csv to df\n",
    "    for filename in os.listdir(directory):\n",
    "        \n",
    "        if filename.endswith('.zst'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            try:\n",
    "                # Open the compressed file in binary mode\n",
    "                with open(filepath, 'rb') as compressed_file:\n",
    "                    # Initialize the decompressor\n",
    "                    dctx = zstd.ZstdDecompressor()\n",
    "                    \n",
    "                    # Decompress the file into an in-memory buffer\n",
    "                    with dctx.stream_reader(compressed_file) as decompressed_stream:\n",
    "                        text_stream = io.TextIOWrapper(decompressed_stream, encoding='utf-8')\n",
    "                        print(f\"Processing {filename}\\n\")\n",
    "                        # Read the decompressed data into a pandas DataFrame\n",
    "                        df = pd.read_csv(text_stream, parse_dates=['ts_recv', 'ts_event'])\n",
    "                        print(f\"Processed {filename}\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}\\n\")\n",
    "                print(e)\n",
    "                continue\n",
    "                \n",
    "            ret.append(df)\n",
    "            \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e6af0dd-113c-478b-a31e-499ef2751456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing xnas-itch-20240823.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240823.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240830.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240830.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240822.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240822.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240828.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240828.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240820.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240820.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240827.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240827.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240826.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240826.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240821.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240821.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240829.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240829.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240815.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240815.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240808.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240808.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240807.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240807.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240812.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240812.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240813.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240813.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240806.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240806.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240809.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240809.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240801.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240801.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240814.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240814.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240816.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240816.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240819.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240819.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240805.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240805.mbp-10.csv.zst\n",
      "\n",
      "Processing xnas-itch-20240802.mbp-10.csv.zst\n",
      "\n",
      "Processed xnas-itch-20240802.mbp-10.csv.zst\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs = process_csv_files('equity-data', 'ANF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "744be5dc-bf51-4ee5-ba24-86d0c447757e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 saved file as pickle\n",
      "2 saved file as pickle\n",
      "3 saved file as pickle\n",
      "4 saved file as pickle\n",
      "5 saved file as pickle\n",
      "6 saved file as pickle\n",
      "7 saved file as pickle\n",
      "8 saved file as pickle\n",
      "9 saved file as pickle\n",
      "10 saved file as pickle\n",
      "11 saved file as pickle\n",
      "12 saved file as pickle\n",
      "13 saved file as pickle\n",
      "14 saved file as pickle\n",
      "15 saved file as pickle\n",
      "16 saved file as pickle\n",
      "17 saved file as pickle\n",
      "18 saved file as pickle\n",
      "19 saved file as pickle\n",
      "20 saved file as pickle\n",
      "21 saved file as pickle\n",
      "22 saved file as pickle\n"
     ]
    }
   ],
   "source": [
    "# Save down as pkl file - for use in case user needs to reset kernel, can fetch back dataframes faster\n",
    "day_df = {}\n",
    "\n",
    "i = 1\n",
    "for df in dfs:\n",
    "    df.to_pickle(\"day_\" + str(i) + \".pkl\")\n",
    "    print(str(i) + ' saved file as pickle')\n",
    "    i +=1\n",
    "    day_df[i] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c71bbc-ff38-40b2-bdac-a80f71fabc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded day_19.pkl\n",
      "Loaded day_18.pkl\n",
      "Loaded day_20.pkl\n",
      "Loaded day_21.pkl\n",
      "Loaded day_22.pkl\n",
      "Loaded day_9.pkl\n",
      "Loaded day_8.pkl\n",
      "Loaded day_6.pkl\n",
      "Loaded day_7.pkl\n",
      "Loaded day_5.pkl\n",
      "Loaded day_4.pkl\n",
      "Loaded day_1.pkl\n",
      "Loaded day_3.pkl\n",
      "Loaded day_2.pkl\n",
      "Loaded day_13.pkl\n",
      "Loaded day_12.pkl\n",
      "Loaded day_10.pkl\n",
      "Loaded day_11.pkl\n",
      "Loaded day_15.pkl\n",
      "Loaded day_14.pkl\n",
      "Loaded day_16.pkl\n",
      "Loaded day_17.pkl\n"
     ]
    }
   ],
   "source": [
    "directory = 'equity-pkl'\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.pkl'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            dfs.append(data)\n",
    "            print(f\"Loaded {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2de5fb3-fc3f-4252-a298-ff15dce7513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n",
      "15\n",
      "15\n",
      "16\n",
      "16\n",
      "17\n",
      "17\n",
      "18\n",
      "18\n",
      "19\n",
      "19\n",
      "20\n",
      "20\n",
      "21\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "bbo_dfs_nvda = []\n",
    "bbo_dfs_anf = []\n",
    "\n",
    "tickers = ['NVDA', 'ANF']\n",
    "\n",
    "i = 0\n",
    "for df in dfs:\n",
    "    for tkr in tickers:\n",
    "        print(i)\n",
    "        if 'ts_event' in list(df.columns):\n",
    "            df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "            # Set 'ts_event' as index\n",
    "            df = df.set_index('ts_event')\n",
    "                    \n",
    "        bbo_df = df.between_time('13:40', '19:55', inclusive='left')\n",
    "        \n",
    "        tkr_df_nvda = bbo_df[bbo_df['symbol'] == 'NVDA'].resample('100ms').last().ffill()\n",
    "        tkr_df_anf = bbo_df[bbo_df['symbol'] == 'ANF'].resample('100ms').last().ffill()\n",
    "\n",
    "        bbo_dfs_nvda.append(tkr_df_nvda)\n",
    "        bbo_dfs_anf.append(tkr_df_anf)\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc13649-fbaa-450d-9b86-441795d8f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_signal(df, short_window, long_window, b):\n",
    "    \"\"\"\n",
    "    Generate trading signals based on moving average cross-over strategy.\n",
    "\n",
    "    Params:\n",
    "    prices (Series): asset prices\n",
    "    short_window (int): Window size for the short-term moving average\n",
    "    long_window (int): Window size for the long-term moving average\n",
    "    b (int): Bandwidth parameter that determines the buy/sell thresholds\n",
    "\n",
    "    Return:\n",
    "    return (Series): Series with trading signals (+1 for buy, -1 for sell, 0 for hold)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create midprice column (average of bid and ask prices)\n",
    "    mid_price = (df['bid_px_00'] + df['ask_px_00']) / 2\n",
    "    \n",
    "    # Calculate short-term and long-term moving averages\n",
    "    short_ma = mid_price.rolling(window=short_window).mean()\n",
    "    long_ma = mid_price.rolling(window=long_window).mean()\n",
    "    \n",
    "    # Define thresholds\n",
    "    upper_threshold = (1 + b) * long_ma\n",
    "    lower_threshold = (1 - b) * long_ma\n",
    "    \n",
    "    signal = pd.Series(0, index=df.index)\n",
    "\n",
    "    # Generate buy signals (+1 where short_ma > upper_threshold)\n",
    "    signal[short_ma > upper_threshold] = 1\n",
    "    \n",
    "    # Generate sell signals (-1 where short_ma < lower_threshold)\n",
    "    signal[short_ma < lower_threshold] = -1\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bd4610b-c290-46ab-86e4-27d2a5450d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_order_book(bids, asks, bid_price, bid_size, ask_price, ask_size):\n",
    "    \"\"\"\n",
    "    Update the 5-level deep order book with new bid and ask data.\n",
    "\n",
    "    Params:\n",
    "     bids (DataFrame): DataFrame maintaining top 5 bid prices and sizes.\n",
    "     asks (DataFrame): DataFrame maintaining top 5 ask prices and sizes.\n",
    "     bid_price (float): New bid price.\n",
    "     bid_size (int): New bid size.\n",
    "     ask_price (float): New ask price.\n",
    "     ask_size (int): New ask size.\n",
    "\n",
    "    Returns:\n",
    "    - Updated bids and asks DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Update bids\n",
    "    if bid_price > 0 and bid_size > 0:\n",
    "        if (bids['price'] == bid_price).any():\n",
    "            # If bid price already exists, update the size\n",
    "            bids.loc[bids['price'] == bid_price, 'size'] = bid_size\n",
    "        else:\n",
    "            # If it's a new bid price, add it and sort\n",
    "            new_bid = pd.DataFrame({'price': [bid_price], 'size': [bid_size]})\n",
    "            bids = pd.concat([bids, new_bid]).nlargest(5, 'price').reset_index(drop=True)\n",
    "\n",
    "    # Update asks\n",
    "    if ask_price > 0 and ask_size > 0:\n",
    "        if (asks['price'] == ask_price).any():\n",
    "            # If ask price already exists, update the size\n",
    "            asks.loc[asks['price'] == ask_price, 'size'] = ask_size\n",
    "        else:\n",
    "            # If it's a new ask price, add it and sort\n",
    "            new_ask = pd.DataFrame({'price': [ask_price], 'size': [ask_size]})\n",
    "            asks = pd.concat([asks, new_ask]).nsmallest(5, 'price').reset_index(drop=True)\n",
    "\n",
    "    return bids, asks\n",
    "\n",
    "def process_mpb10_data(mpb10_df):\n",
    "    \"\"\"\n",
    "    Process MPB-10 data to maintain a 5-level deep order book.\n",
    "\n",
    "    Params:\n",
    "    - mpb10_df (DataFrame): DataFrame containing the MPB-10 data.\n",
    "\n",
    "    Returns:\n",
    "    - bids (DataFrame): Final top 5 bid prices and sizes.\n",
    "    - asks (DataFrame): Final top 5 ask prices and sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize empty DataFrames for the top 5 levels of bids and asks\n",
    "    bids = pd.DataFrame(columns=['price', 'size'], index=range(5))\n",
    "    asks = pd.DataFrame(columns=['price', 'size'], index=range(5))\n",
    "\n",
    "    for index, row in mpb10_df.iterrows():\n",
    "        # Extract bid and ask prices and sizes from each row\n",
    "        bid_price = row['bid_px_00']\n",
    "        bid_size = row['bid_sz_00']\n",
    "        ask_price = row['ask_px_00']\n",
    "        ask_size = row['ask_sz_00']\n",
    "\n",
    "        # Update the order book\n",
    "        bids, asks = update_order_book(bids, asks, bid_price, bid_size, ask_price, ask_size)\n",
    "\n",
    "    return bids, asks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "025ed679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_trading_signal(bbo_df, signal, order_size=1):\n",
    "    \"\"\"\n",
    "    Execute the trading signal based on the available order book depth and calculate actual P&L.\n",
    "\n",
    "    bbo_df (DataFrame): DataFrame containing the order book and signals for each day.\n",
    "    signal (Series): Generated trading signals (+1 for buy, -1 for sell, 0 for hold).\n",
    "    order_size (int): Size of the order (e.g., $1M or 1 share for initial).\n",
    "\n",
    "    :return: DataFrame with actual P&L calculated for each signal.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize return and position tracking\n",
    "    bbo_df['return'] = 0\n",
    "    position = 0  # Tracks the position (positive for buy, negative for sell)\n",
    "    \n",
    "    # Loop through the DataFrame and execute trades based on the signal\n",
    "    for i in range(1, len(bbo_df)):\n",
    "        current_signal = signal.iloc[i]\n",
    "        previous_signal = signal.iloc[i-1]\n",
    "        \n",
    "        if current_signal != previous_signal:  # Only act when signal changes\n",
    "            # Extract the order book state\n",
    "            bids, asks = process_mpb10_data(bbo_df.iloc[:i+1])\n",
    "\n",
    "            # Trade on buy signal (+1) and sell signal (-1)\n",
    "            if current_signal == 1 and position == 0:\n",
    "                # Buy at the best ask price, fill up to the order size\n",
    "                best_ask_price = asks.iloc[0]['price']\n",
    "                position = order_size\n",
    "                bbo_df.loc[bbo_df.index[i], 'return'] = -best_ask_price  # negative because we are buying\n",
    "                \n",
    "            elif current_signal == -1 and position > 0:\n",
    "                # Sell at the best bid price, close the position\n",
    "                best_bid_price = bids.iloc[0]['price']\n",
    "                bbo_df.loc[bbo_df.index[i], 'return'] = (best_bid_price - abs(bbo_df['return'][i-1]))  # sell return\n",
    "                position = 0  # Reset position after selling\n",
    "\n",
    "    # Scale return to $1M for actual P&L calculation as per assignment description\n",
    "    bbo_df['scaled_return'] = bbo_df['return'] * 1_000_000\n",
    "    \n",
    "    # Cumulative returns for analysis\n",
    "    bbo_df['cumulative_return'] = bbo_df['scaled_return'].cumsum()\n",
    "    \n",
    "    return bbo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a92c6feb-5a9a-4bc8-8abe-f110f9d32c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = bbo_dfs_anf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061d077-ee58-4bd3-b3f5-2a42817c4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = moving_average_signal(test, short_window=50, long_window=500, b=0.0001)\n",
    "\n",
    "returns = execute_trading_signal(test, signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08affd-39d8-451b-9bb2-491550c00790",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns['cumulative_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ced212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# theoretical, hopefully this works\n",
    "\n",
    "returns = []\n",
    "for df in bbo_dfs_anf:\n",
    "    ret = execute_trading_signal(df, signal)\n",
    "    returns.append(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31047294-4819-4760-9e18-ab33fc25f177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
