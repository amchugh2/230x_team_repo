{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85a6c225-8666-433e-825d-04860658289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import zstandard as zstd\n",
    "import io\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0280e0-f5f7-476a-9416-6b03673ab310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ea7205-21de-4963-b3b1-8c59ec9adc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(directory, ticker):\n",
    "    \"\"\"\n",
    "    Convert input csv to dataframes and prepare to process through trading signal.\n",
    "\n",
    "    Params:\n",
    "    directory (String): Name of directory that contains list of csv files.\n",
    "\n",
    "    Returns:\n",
    "    return (list[DataFrame]): List of pandas DataFrames, one for each trading day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Directory containing the CSV files\n",
    "    \n",
    "    ret = []\n",
    "    i = 1\n",
    "    # Loop through all files in the directory and convert .zst csv to df\n",
    "    for filename in os.listdir(directory):\n",
    "        \n",
    "        if filename.endswith('.zst'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            try:\n",
    "                # Open the compressed file in binary mode\n",
    "                with open(filepath, 'rb') as compressed_file:\n",
    "                    # Initialize the decompressor\n",
    "                    dctx = zstd.ZstdDecompressor()\n",
    "                    \n",
    "                    # Decompress the file into an in-memory buffer\n",
    "                    with dctx.stream_reader(compressed_file) as decompressed_stream:\n",
    "                        text_stream = io.TextIOWrapper(decompressed_stream, encoding='utf-8')\n",
    "                        print(f\"Processing {filename}\\n\")\n",
    "                        # Read the decompressed data into a pandas DataFrame\n",
    "                        df = pd.read_csv(text_stream, parse_dates=['ts_recv', 'ts_event'])\n",
    "                        print(str(i) + ' read file')\n",
    "                        df.to_pickle(os.path.join(directory,\"day_\" + str(i) + \".pkl\"))\n",
    "                        print(str(i) + ' saved file as pickle')\n",
    "                        i +=1\n",
    "                        print(f\"Processed {filename}\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}\\n\")\n",
    "                print(e)\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8024aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files_uncompressed(directory, ticker):\n",
    "    \"\"\"\n",
    "    Convert input csv to dataframes and prepare to process through trading signal.\n",
    "\n",
    "    Params:\n",
    "    directory (String): Name of directory that contains list of csv files.\n",
    "\n",
    "    Returns:\n",
    "    return (list[DataFrame]): List of pandas DataFrames, one for each trading day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Directory containing the CSV files\n",
    "    \n",
    "    ret = []\n",
    "    i = 1\n",
    "    # Loop through all files in the directory and convert .zst csv to df\n",
    "    for filename in os.listdir(directory):\n",
    "        \n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            try:\n",
    "                print(f\"Processing {filename}\\n\")\n",
    "                # Read the decompressed data into a pandas DataFrame\n",
    "                df = pd.read_csv(filepath, parse_dates=['ts_recv', 'ts_event'])\n",
    "                print(str(i) + ' read file')\n",
    "                df.to_pickle(os.path.join(directory,\"day_\" + str(i) + \".pkl\"))\n",
    "                print(str(i) + ' saved file as pickle')\n",
    "                i +=1\n",
    "                print(f\"Processed {filename}\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}\\n\")\n",
    "                print(e)\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e6af0dd-113c-478b-a31e-499ef2751456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing xnas-itch-20240819.mbp-10.csv.zst\n",
      "\n",
      "1 read file\n",
      "1 saved file as pickle\n",
      "Processed xnas-itch-20240819.mbp-10.csv.zst\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs = process_csv_files('test-dir', 'ANF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "744be5dc-bf51-4ee5-ba24-86d0c447757e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 saved file as pickle\n",
      "2 saved file as pickle\n",
      "3 saved file as pickle\n",
      "4 saved file as pickle\n",
      "5 saved file as pickle\n",
      "6 saved file as pickle\n",
      "7 saved file as pickle\n",
      "8 saved file as pickle\n",
      "9 saved file as pickle\n",
      "10 saved file as pickle\n",
      "11 saved file as pickle\n",
      "12 saved file as pickle\n",
      "13 saved file as pickle\n",
      "14 saved file as pickle\n",
      "15 saved file as pickle\n",
      "16 saved file as pickle\n",
      "17 saved file as pickle\n",
      "18 saved file as pickle\n",
      "19 saved file as pickle\n",
      "20 saved file as pickle\n",
      "21 saved file as pickle\n",
      "22 saved file as pickle\n"
     ]
    }
   ],
   "source": [
    "# Save down as pkl file - for use in case user needs to reset kernel, can fetch back dataframes faster\n",
    "day_df = {}\n",
    "\n",
    "i = 1\n",
    "for df in dfs:\n",
    "    df.to_pickle(\"day_\" + str(i) + \".pkl\")\n",
    "    print(str(i) + ' saved file as pickle')\n",
    "    i +=1\n",
    "    day_df[i] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c71bbc-ff38-40b2-bdac-a80f71fabc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded day_19.pkl\n",
      "Loaded day_18.pkl\n",
      "Loaded day_20.pkl\n",
      "Loaded day_21.pkl\n",
      "Loaded day_22.pkl\n",
      "Loaded day_9.pkl\n",
      "Loaded day_8.pkl\n",
      "Loaded day_6.pkl\n",
      "Loaded day_7.pkl\n",
      "Loaded day_5.pkl\n",
      "Loaded day_4.pkl\n",
      "Loaded day_1.pkl\n",
      "Loaded day_3.pkl\n",
      "Loaded day_2.pkl\n",
      "Loaded day_13.pkl\n",
      "Loaded day_12.pkl\n",
      "Loaded day_10.pkl\n",
      "Loaded day_11.pkl\n",
      "Loaded day_15.pkl\n",
      "Loaded day_14.pkl\n",
      "Loaded day_16.pkl\n",
      "Loaded day_17.pkl\n"
     ]
    }
   ],
   "source": [
    "directory = 'equity-pkl'\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.pkl'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            dfs.append(data)\n",
    "            print(f\"Loaded {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2de5fb3-fc3f-4252-a298-ff15dce7513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data\n",
    "# directory = 'equity-pkl'\n",
    "# bbo_dfs_nvda = []\n",
    "# bbo_dfs_anf = []\n",
    "\n",
    "# tickers = ['NVDA', 'ANF']\n",
    "\n",
    "# i = 0\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.endswith('.pkl'):\n",
    "#         filepath = os.path.join(directory, filename)\n",
    "#         with open(filepath, 'rb') as file:\n",
    "#             df = pickle.load(file)\n",
    "#         for tkr in tickers:\n",
    "#             print(i)\n",
    "#             if 'ts_event' in list(df.columns):\n",
    "#                 # df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "#                 # Set 'ts_event' as index\n",
    "#                 df = df.set_index('ts_event')\n",
    "                        \n",
    "#             bbo_df = df.between_time('13:40', '19:55', inclusive='left')\n",
    "            \n",
    "#             tkr_df_nvda = bbo_df[bbo_df['symbol'] == 'NVDA'].resample('100ms').last().ffill()\n",
    "#             tkr_df_anf = bbo_df[bbo_df['symbol'] == 'ANF'].resample('100ms').last().ffill()\n",
    "\n",
    "#             #do not concatenate dataframes here - this causes kernel shutdown\n",
    "\n",
    "\n",
    "#             bbo_dfs_nvda.append(tkr_df_nvda)\n",
    "#             bbo_dfs_anf.append(tkr_df_anf)\n",
    "\n",
    "#         i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efc13649-fbaa-450d-9b86-441795d8f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_signal(df, short_window, long_window, b,delay=0):\n",
    "    \"\"\"\n",
    "    Generate trading signals based on moving average cross-over strategy.\n",
    "\n",
    "    Params:\n",
    "    prices (Series): asset prices\n",
    "    short_window (int): Window size for the short-term moving average\n",
    "    long_window (int): Window size for the long-term moving average\n",
    "    b (int): Bandwidth parameter that determines the buy/sell thresholds\n",
    "\n",
    "    Return:\n",
    "    return (Series): Series with trading signals (+1 for buy, -1 for sell, 0 for hold)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create midprice column (average of bid and ask prices)\n",
    "    mid_price = (df['bid_px_00'] + df['ask_px_00']) / 2\n",
    "    \n",
    "    # Calculate short-term and long-term moving averages\n",
    "    short_ma = mid_price.rolling(window=short_window).mean()\n",
    "    long_ma = mid_price.rolling(window=long_window).mean()\n",
    "    \n",
    "    # Define thresholds\n",
    "    upper_threshold = (1 + b) * long_ma\n",
    "    lower_threshold = (1 - b) * long_ma\n",
    "    \n",
    "    signal = pd.Series(0, index=df.index)\n",
    "\n",
    "    # Generate buy signals (+1 where short_ma > upper_threshold)\n",
    "    signal[short_ma > upper_threshold] = 1\n",
    "    \n",
    "    # Generate sell signals (-1 where short_ma < lower_threshold)\n",
    "    signal[short_ma < lower_threshold] = -1\n",
    "\n",
    "    # Delay signal by 'delay' periods to account for trading frictions\n",
    "    signal = signal.shift(delay).fillna(0)\n",
    "\n",
    "    #Set signal to 0 at the end of the day to ensure no overnight positions\n",
    "    signal[df.index.time > pd.to_datetime('19:55').time()] = 0\n",
    "\n",
    "    return signal\n",
    "\n",
    "def momentum_in_price_signal(df, short_window, long_window, b,delay=0):\n",
    "    \"\"\"\n",
    "    Generate trading signals based on momentum in price strategy.\n",
    "\n",
    "    Params:\n",
    "    prices (Series): asset prices\n",
    "    short_window (int): Window size for the short-term moving average\n",
    "    long_window (int): Window size for the long-term moving average\n",
    "    b (int): Bandwidth parameter that determines the buy/sell thresholds\n",
    "\n",
    "    Return:\n",
    "    return (Series): Series with trading signals (+1 for buy, -1 for sell, 0 for hold)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create midprice column (average of bid and ask prices)\n",
    "    mid_price = (df['bid_px_00'] + df['ask_px_00']) / 2\n",
    "    returns = mid_price.pct_change()\n",
    "\n",
    "    # Calculate short-term and long-term moving averages\n",
    "    short_ma = returns.rolling(window=short_window).mean()\n",
    "    long_ma = returns.rolling(window=long_window).mean()\n",
    "    \n",
    "    # Define thresholds\n",
    "    upper_threshold = (1 + b) * long_ma\n",
    "    lower_threshold = (1 - b) * long_ma\n",
    "    \n",
    "    signal = pd.Series(0, index=df.index)\n",
    "\n",
    "    # Generate buy signals (+1 where short_ma > upper_threshold)\n",
    "    signal[short_ma > upper_threshold] = 1\n",
    "    \n",
    "    # Generate sell signals (-1 where short_ma < lower_threshold)\n",
    "    signal[short_ma < lower_threshold] = -1\n",
    "\n",
    "    # Delay signal by 'delay' periods to account for trading frictions\n",
    "    signal = signal.shift(delay).fillna(0)\n",
    "\n",
    "    #Set signal to 0 at the end of the day to ensure no overnight positions\n",
    "    signal[df.index.time > pd.to_datetime('19:55').time()] = 0\n",
    "\n",
    "    return signal\n",
    "\n",
    "def spread_order_imbalance(df, short_window,long_window, delay=0):\n",
    "    \"\"\"\n",
    "    Generate trading signals based on spread order imbalance strategy.\n",
    "\n",
    "    Params:\n",
    "    df (DataFrame): DataFrame containing bid and ask prices and volumes\n",
    "    delay (int): Number of periods to delay the signal\n",
    "\n",
    "    Return:\n",
    "    return (Series): Series with trading signals (+1 for buy, -1 for sell, 0 for hold)\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate spread\n",
    "    spread = df['ask_px_00'] - df['bid_px_00']\n",
    "\n",
    "    # Calculate short-term and long-term moving averages of spread\n",
    "    spread_ma_short= spread.rolling(window=short_window).mean()\n",
    "    spread_ma_long = spread.rolling(window=long_window).mean()\n",
    "\n",
    "    \n",
    "    # Calculate order imbalance\n",
    "    order_imbalance = (df['ask_sz_00'] - df['bid_sz_00']) / (df['ask_sz_00'] + df['bid_sz_00'])\n",
    "    \n",
    "    # Generate signal based on order imbalance and spread\n",
    "    signal = pd.Series(0, index=df.index)\n",
    "    signal[(spread_ma_short<spread_ma_long)&(order_imbalance > 0.5)] = 1\n",
    "    signal[(spread_ma_short>spread_ma_long)&(order_imbalance < -0.5)] = -1\n",
    "\n",
    "    # Delay signal by 'delay' periods to account for trading frictions\n",
    "    signal = signal.shift(delay).fillna(0)\n",
    "\n",
    "    #Set signal to 0 at the end of the day to ensure no overnight positions\n",
    "    signal[df.index.time > pd.to_datetime('19:55').time()] = 0\n",
    "\n",
    "    return signal\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd4610b-c290-46ab-86e4-27d2a5450d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is not used anywhere\n",
    "def update_order_book(bids, asks, bid_price, bid_size, ask_price, ask_size):\n",
    "    \"\"\"\n",
    "    Update the 5-level deep order book with new bid and ask data.\n",
    "    Params:\n",
    "     bids (DataFrame): DataFrame maintaining top 3 bid prices and sizes.\n",
    "     asks (DataFrame): DataFrame maintaining top 3 ask prices and sizes.\n",
    "     bid_price (float): New bid price.\n",
    "     bid_size (int): New bid size.\n",
    "     ask_price (float): New ask price.\n",
    "     ask_size (int): New ask size.\n",
    "\n",
    "    Returns:\n",
    "    - Updated bids and asks DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Update bids\n",
    "    if bid_price > 0 and bid_size > 0:\n",
    "        if (bids['price'] == bid_price).any():\n",
    "            # If bid price already exists, update the size\n",
    "            bids.loc[bids['price'] == bid_price, 'size'] = bid_size\n",
    "        else:\n",
    "            # If it's a new bid price, add it and sort\n",
    "            new_bid = pd.DataFrame({'price': [bid_price], 'size': [bid_size]})\n",
    "            bids = pd.concat([bids, new_bid]).nlargest(3, 'price').reset_index(drop=True)\n",
    "\n",
    "    # Update asks\n",
    "    if ask_price > 0 and ask_size > 0:\n",
    "        if (asks['price'] == ask_price).any():\n",
    "            # If ask price already exists, update the size\n",
    "            asks.loc[asks['price'] == ask_price, 'size'] = ask_size\n",
    "        else:\n",
    "            # If it's a new ask price, add it and sort\n",
    "            new_ask = pd.DataFrame({'price': [ask_price], 'size': [ask_size]})\n",
    "            asks = pd.concat([asks, new_ask]).nsmallest(3, 'price').reset_index(drop=True)\n",
    "\n",
    "    return bids, asks\n",
    "\n",
    "def process_mpb10_data(mpb10_df):\n",
    "    \"\"\"\n",
    "    Process MPB-10 data to maintain a 3-level deep order book for each timestamp.\n",
    "\n",
    "    Params:\n",
    "    - mpb10_df (DataFrame): DataFrame containing the MPB-10 data with ts_event as index.\n",
    "\n",
    "    Returns:\n",
    "    - bids (DataFrame): DataFrame of top 3 bid prices and sizes indexed by ts_event.\n",
    "    - asks (DataFrame): DataFrame of top 3 ask prices and sizes indexed by ts_event.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize empty DataFrames for the top 3 levels of bids and asks\n",
    "    bids = pd.DataFrame(columns=['ts_event', 'bid_px_00', 'bid_sz_00', 'bid_px_01', 'bid_sz_01', 'bid_px_02', 'bid_sz_02'])\n",
    "    asks = pd.DataFrame(columns=['ts_event', 'ask_px_00', 'ask_sz_00', 'ask_px_01', 'ask_sz_01', 'ask_px_02', 'ask_sz_02'])\n",
    "\n",
    "    for index, row in mpb10_df.iterrows():\n",
    "        # Extract bid and ask prices and sizes from each row\n",
    "        bid_data = {\n",
    "            'ts_event': index,\n",
    "            'bid_px_00': row['bid_px_00'], 'bid_sz_00': row['bid_sz_00'],\n",
    "            'bid_px_01': row['bid_px_01'], 'bid_sz_01': row['bid_sz_01'],\n",
    "            'bid_px_02': row['bid_px_02'], 'bid_sz_02': row['bid_sz_02']\n",
    "        }\n",
    "\n",
    "        ask_data = {\n",
    "            'ts_event': index,\n",
    "            'ask_px_00': row['ask_px_00'], 'ask_sz_00': row['ask_sz_00'],\n",
    "            'ask_px_01': row['ask_px_01'], 'ask_sz_01': row['ask_sz_01'],\n",
    "            'ask_px_02': row['ask_px_02'], 'ask_sz_02': row['ask_sz_02']\n",
    "        }\n",
    "\n",
    "        # Use concat instead of append to add rows to bids and asks DataFrames\n",
    "        bids = pd.concat([bids, pd.DataFrame([bid_data])], ignore_index=True)\n",
    "        asks = pd.concat([asks, pd.DataFrame([ask_data])], ignore_index=True)\n",
    "\n",
    "    # Set 'ts_event' as the index for both bids and asks\n",
    "    bids.set_index('ts_event', inplace=True)\n",
    "    asks.set_index('ts_event', inplace=True)\n",
    "\n",
    "    return bids, asks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "025ed679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_trading_signal(df, signal, order_size=1_000_000):\n",
    "    \"\"\"\n",
    "    Execute the trading signal based on the available order book depth and calculate actual P&L.\n",
    "    \n",
    "    Params:\n",
    "    bbo_df (DataFrame): DataFrame containing the order book and signals for each day.\n",
    "    signal (Series): Series containing the generated trading signals.\n",
    "    order_size (float): Size of the order for each trade (default $1M).\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Updated DataFrame with calculated returns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize position and return\n",
    "    position = 0\n",
    "    shares_held = 0\n",
    "    bbo_df = df.copy()\n",
    "    bbo_df['return'] = 0\n",
    "\n",
    "    # Process MPB-10 data to get 3-level deep order book\n",
    "    #bids, asks = process_mpb10_data(bbo_df)\n",
    "    bids = bbo_df[['bid_px_00', 'bid_sz_00', 'bid_px_01', 'bid_sz_01', 'bid_px_02', 'bid_sz_02']].copy()\n",
    "    asks = bbo_df[['ask_px_00', 'ask_sz_00', 'ask_px_01', 'ask_sz_01', 'ask_px_02', 'ask_sz_02']].copy()\n",
    "\n",
    "    # Track the first signal to open a position, then exit when the signal reverses\n",
    "    open_position = False\n",
    "    \n",
    "    for i in signal.index:\n",
    "        current_signal = signal.loc[i]\n",
    "        \n",
    "        # Buy logic: first time signal turns +1 and we don't have an open position\n",
    "        if current_signal == 1 and (not open_position or shares_held < 0):\n",
    "            shares_bought = 0\n",
    "            investment = 0\n",
    "\n",
    "            # Buy up to 3 levels of the ask side until $1M is spent\n",
    "            for level in range(3):\n",
    "                ask_price = asks.loc[i, f'ask_px_0{level}']\n",
    "                ask_size = asks.loc[i, f'ask_sz_0{level}']\n",
    "                level_investment = min(order_size - shares_held*ask_price - investment, ask_price * ask_size)\n",
    "\n",
    "                # Calculate number of shares to buy\n",
    "                level_shares = level_investment / ask_price\n",
    "                shares_bought += level_shares\n",
    "                investment += level_investment\n",
    "                \n",
    "                if investment >= order_size:\n",
    "                    break  # Exit loop when $1M investment is reached\n",
    "            \n",
    "            position += investment\n",
    "            shares_held += shares_bought\n",
    "            open_position = True  # Mark that position is open\n",
    "            \n",
    "            # Mark the negative cash flow from buying\n",
    "            bbo_df.loc[i, 'return'] = -investment\n",
    "            \n",
    "        # Sell logic: first time signal turns -1 and we don't have an open position\n",
    "        elif current_signal == -1 and (not open_position or shares_held > 0):\n",
    "            shares_sold = 0\n",
    "            revenue = 0\n",
    "\n",
    "            # Sell up to 3 levels of the bid side, until $1M worth of shares are sold\n",
    "            for level in range(3):\n",
    "                bid_price = bids.loc[i, f'bid_px_0{level}']\n",
    "                bid_size = bids.loc[i, f'bid_sz_0{level}']\n",
    "                level_revenue = min(order_size + bid_price*shares_held - revenue, bid_price * bid_size)\n",
    "                level_shares = level_revenue / bid_price\n",
    "\n",
    "                shares_sold += level_shares\n",
    "                revenue += level_revenue\n",
    "                \n",
    "                if revenue >= order_size:\n",
    "                    break\n",
    "            \n",
    "            position -= revenue\n",
    "            shares_held -= shares_sold\n",
    "            open_position = True\n",
    "\n",
    "            # Mark the positive cash flow from selling\n",
    "            bbo_df.loc[i, 'return'] = revenue\n",
    "\n",
    "        \n",
    "        # Exit logic: signal turns 0 and we have an open position\n",
    "        elif current_signal == 0 and open_position:\n",
    "            # Liquidate the position by settling the open trade\n",
    "            if shares_held > 0:\n",
    "                # Sell all shares at the bid price\n",
    "                ask_price = asks.loc[i, 'ask_px_00']\n",
    "                ask_size = asks.loc[i, 'ask_sz_00']\n",
    "                sell_qty = min(shares_held, bid_size)\n",
    "                revenue = sell_qty * ask_price\n",
    "                position -= revenue\n",
    "                shares_held -= sell_qty\n",
    "\n",
    "                if(shares_held == 0):\n",
    "                    open_position = False\n",
    "\n",
    "                # Mark the positive cash flow from selling\n",
    "                bbo_df.loc[i, 'return'] = revenue\n",
    "\n",
    "            elif shares_held < 0:\n",
    "                # Buy back all shares at the ask price\n",
    "                bid_price = bids.loc[i, 'bid_px_00']\n",
    "                bid_size = bids.loc[i, 'bid_sz_00']\n",
    "                buy_qty = min(-shares_held, bid_size)\n",
    "                investment = buy_qty * bid_price\n",
    "                position += investment\n",
    "                shares_held += buy_qty\n",
    "                if(shares_held == 0):\n",
    "                    open_position = False\n",
    "\n",
    "                # Mark the negative cash flow from buying\n",
    "                bbo_df.loc[i, 'return'] = -investment\n",
    "\n",
    "    # Scale the cumulative return\n",
    "    bbo_df['cumulative_return'] = (bbo_df['return']).cumsum()\n",
    "    bbo_df['scaled_return'] = bbo_df['cumulative_return'] / 1_000_000\n",
    "    \n",
    "    return bbo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a92c6feb-5a9a-4bc8-8abe-f110f9d32c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = bbo_dfs_anf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb9ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data\n",
    "directory = 'equity-pkl'\n",
    "bbo_dfs_nvda = []\n",
    "bbo_dfs_anf = []\n",
    "\n",
    "tickers = ['NVDA', 'ANF']\n",
    "\n",
    "def signal_and_trade(ticker, directory):\n",
    "    \n",
    "    returns_cumulative = pd.Series()\n",
    "    i = 0\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pkl'):\n",
    "            print('Processing ' + filename)\n",
    "            \n",
    "            filepath = os.path.join(directory, filename)\n",
    "            \n",
    "            with open(filepath, 'rb') as file:\n",
    "                df = pickle.load(file)\n",
    "                \n",
    "            print('Successfully loaded pkl file. Processing as dataframe.')\n",
    "            \n",
    "            if 'ts_event' in list(df.columns):\n",
    "                df = df.set_index('ts_event')\n",
    "                        \n",
    "            bbo_df = df.between_time('13:40', '19:55', inclusive='left')\n",
    "            tkr_df= bbo_df[bbo_df['symbol'] == ticker].resample('100ms').last().ffill()\n",
    "\n",
    "            print('Successfully cleaned DataFrame for processing. Sending to generate returns.')\n",
    "            \n",
    "            signal= moving_average_signal(tkr_df, 50, 500, 0.001)\n",
    "\n",
    "            print('Successfully generated trading signal. Sending to generate returns.')\n",
    "            \n",
    "            returns = execute_trading_signal(tkr_df, signal)\n",
    "\n",
    "            print('Returns sucessfully generated for ' + filename, '\\n')\n",
    "            \n",
    "            i+=1\n",
    "            \n",
    "    return returns\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b7f4327-5913-4ab1-b7e4-589562624cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing day_1.pkl\n",
      "Successfully loaded pkl file. Processing as dataframe.\n",
      "Successfully generated trading signal. Sending to generate returns.\n",
      "Returns sucessfully generated for day_1.pkl \n",
      "\n",
      "Moving average return:958.280000000059\n",
      "Momentum return:-5241.880000000005\n",
      "Combined return:-191346.7499999999\n"
     ]
    }
   ],
   "source": [
    "# for testing purposes to speed up debugging\n",
    "i = 0\n",
    "directory='test-dir'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.pkl'):\n",
    "        print('Processing ' + filename)\n",
    "        \n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        with open(filepath, 'rb') as file:\n",
    "            df = pickle.load(file)\n",
    "            \n",
    "        print('Successfully loaded pkl file. Processing as dataframe.')\n",
    "        \n",
    "        if 'ts_event' in list(df.columns):\n",
    "            df = df.set_index('ts_event')\n",
    "                    \n",
    "        bbo_df = df.between_time('13:40', '19:55', inclusive='left')\n",
    "        tkr_df = bbo_df[bbo_df['symbol'] == 'ANF'].resample('5s').last().ffill()\n",
    "\n",
    "        #df = tkr_df.head(500) # to speed up testing\n",
    "\n",
    "        signal_ma = moving_average_signal(tkr_df, 2,5, 0.0005,delay=0)\n",
    "        signal_mp = momentum_in_price_signal(tkr_df, 2,5, 0.0005,delay=0)\n",
    "        signal_combined = spread_order_imbalance(tkr_df,2,5,delay=0)\n",
    "\n",
    "        print('Successfully generated trading signal. Sending to generate returns.')\n",
    "        \n",
    "        returns_0 = execute_trading_signal(tkr_df, signal_ma)\n",
    "        returns_1 = execute_trading_signal(tkr_df, signal_mp)\n",
    "        returns_2 = execute_trading_signal(tkr_df, signal_combined)\n",
    "        \n",
    "        print('Returns sucessfully generated for ' + filename, '\\n')\n",
    "\n",
    "        print(f'Moving average return:{returns_0[\"cumulative_return\"].tail(1).values[0]}')\n",
    "        print(f'Momentum return:{returns_1[\"cumulative_return\"].tail(1).values[0]}')\n",
    "        print(f'Combined return:{returns_2[\"cumulative_return\"].tail(1).values[0]}')\n",
    "        # random_sample = np.sort(random.sample(range(0, len(returns_0)), 100))\n",
    "        # returns_0['cumulative_return'].plot()\n",
    "        # returns_1['cumulative_return'].plot()\n",
    "        # returns_2['cumulative_return'].plot()\n",
    "\n",
    "        i+=1\n",
    "\n",
    "        if i > 3:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01bb1fe3-bb52-4662-a3bb-801d4c4cebd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2130"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((signal_ma==signal_mp)&(signal_mp==0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67649f39-b6a2-4059-be63-ed8c5dee823c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'signal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m signal\n",
      "\u001b[0;31mNameError\u001b[0m: name 'signal' is not defined"
     ]
    }
   ],
   "source": [
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f4bfdc5-1052-49f9-8263-067523141e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated trading signal. Sending to generate returns.\n",
      "200\n",
      "Looking at index  0\n",
      "Looking at index  1\n",
      "Looking at index  2\n",
      "Looking at index  3\n",
      "Looking at index  4\n",
      "Looking at index  5\n",
      "Looking at index  6\n",
      "Looking at index  7\n",
      "Looking at index  8\n",
      "Looking at index  9\n",
      "Looking at index  10\n",
      "Looking at index  11\n",
      "Looking at index  12\n",
      "Looking at index  13\n",
      "Looking at index  14\n",
      "Looking at index  15\n",
      "Looking at index  16\n",
      "Looking at index  17\n",
      "Looking at index  18\n",
      "Looking at index  19\n",
      "Looking at index  20\n",
      "Looking at index  21\n",
      "Looking at index  22\n",
      "Looking at index  23\n",
      "Looking at index  24\n",
      "Looking at index  25\n",
      "Looking at index  26\n",
      "Looking at index  27\n",
      "Looking at index  28\n",
      "Looking at index  29\n",
      "Looking at index  30\n",
      "Looking at index  31\n",
      "Looking at index  32\n",
      "Looking at index  33\n",
      "Looking at index  34\n",
      "Looking at index  35\n",
      "Looking at index  36\n",
      "Looking at index  37\n",
      "Looking at index  38\n",
      "Looking at index  39\n",
      "Looking at index  40\n",
      "Looking at index  41\n",
      "Looking at index  42\n",
      "Looking at index  43\n",
      "Looking at index  44\n",
      "Looking at index  45\n",
      "Looking at index  46\n",
      "Looking at index  47\n",
      "Looking at index  48\n",
      "Looking at index  49\n",
      "Looking at index  50\n",
      "Looking at index  51\n",
      "Looking at index  52\n",
      "Looking at index  53\n",
      "Looking at index  54\n",
      "Looking at index  55\n",
      "Looking at index  56\n",
      "Looking at index  57\n",
      "Looking at index  58\n",
      "Looking at index  59\n",
      "Looking at index  60\n",
      "Looking at index  61\n",
      "Looking at index  62\n",
      "Looking at index  63\n",
      "Looking at index  64\n",
      "Looking at index  65\n",
      "Looking at index  66\n",
      "Looking at index  67\n",
      "Looking at index  68\n",
      "Looking at index  69\n",
      "Looking at index  70\n",
      "Looking at index  71\n",
      "Looking at index  72\n",
      "Looking at index  73\n",
      "Looking at index  74\n",
      "Looking at index  75\n",
      "Looking at index  76\n",
      "Looking at index  77\n",
      "Looking at index  78\n",
      "Looking at index  79\n",
      "Looking at index  80\n",
      "Looking at index  81\n",
      "Looking at index  82\n",
      "Looking at index  83\n",
      "Looking at index  84\n",
      "Looking at index  85\n",
      "Looking at index  86\n",
      "Looking at index  87\n",
      "Looking at index  88\n",
      "Looking at index  89\n",
      "Looking at index  90\n",
      "Looking at index  91\n",
      "Looking at index  92\n",
      "Looking at index  93\n",
      "Looking at index  94\n",
      "Looking at index  95\n",
      "Looking at index  96\n",
      "Looking at index  97\n",
      "Looking at index  98\n",
      "Looking at index  99\n",
      "Looking at index  100\n",
      "Looking at index  101\n",
      "Looking at index  102\n",
      "Looking at index  103\n",
      "Looking at index  104\n",
      "Looking at index  105\n",
      "Looking at index  106\n",
      "Looking at index  107\n",
      "Looking at index  108\n",
      "Looking at index  109\n",
      "Looking at index  110\n",
      "Looking at index  111\n",
      "Looking at index  112\n",
      "Looking at index  113\n",
      "Looking at index  114\n",
      "Looking at index  115\n",
      "Looking at index  116\n",
      "Looking at index  117\n",
      "Looking at index  118\n",
      "Looking at index  119\n",
      "Looking at index  120\n",
      "Looking at index  121\n",
      "Looking at index  122\n",
      "Looking at index  123\n",
      "Looking at index  124\n",
      "Looking at index  125\n",
      "Looking at index  126\n",
      "Looking at index  127\n",
      "Looking at index  128\n",
      "Looking at index  129\n",
      "Looking at index  130\n",
      "Looking at index  131\n",
      "Looking at index  132\n",
      "Looking at index  133\n",
      "Looking at index  134\n",
      "Looking at index  135\n",
      "Looking at index  136\n",
      "Looking at index  137\n",
      "Looking at index  138\n",
      "Looking at index  139\n",
      "Looking at index  140\n",
      "Looking at index  141\n",
      "Looking at index  142\n",
      "Looking at index  143\n",
      "Looking at index  144\n",
      "Looking at index  145\n",
      "Looking at index  146\n",
      "Looking at index  147\n",
      "Looking at index  148\n",
      "Looking at index  149\n",
      "Looking at index  150\n",
      "Looking at index  151\n",
      "Looking at index  152\n",
      "Looking at index  153\n",
      "Looking at index  154\n",
      "Looking at index  155\n",
      "Looking at index  156\n",
      "Looking at index  157\n",
      "Looking at index  158\n",
      "Looking at index  159\n",
      "Looking at index  160\n",
      "Looking at index  161\n",
      "Looking at index  162\n",
      "Looking at index  163\n",
      "Looking at index  164\n",
      "Looking at index  165\n",
      "Looking at index  166\n",
      "Looking at index  167\n",
      "Looking at index  168\n",
      "Looking at index  169\n",
      "Looking at index  170\n",
      "Looking at index  171\n",
      "Looking at index  172\n",
      "Looking at index  173\n",
      "Looking at index  174\n",
      "Looking at index  175\n",
      "Looking at index  176\n",
      "Looking at index  177\n",
      "Looking at index  178\n",
      "Looking at index  179\n",
      "Looking at index  180\n",
      "Looking at index  181\n",
      "Looking at index  182\n",
      "Looking at index  183\n",
      "Looking at index  184\n",
      "Looking at index  185\n",
      "Looking at index  186\n",
      "Looking at index  187\n",
      "Looking at index  188\n",
      "Looking at index  189\n",
      "Looking at index  190\n",
      "Looking at index  191\n",
      "Looking at index  192\n",
      "Looking at index  193\n",
      "Looking at index  194\n",
      "Looking at index  195\n",
      "Looking at index  196\n",
      "Looking at index  197\n",
      "Looking at index  198\n",
      "Looking at index  199\n",
      "Returns sucessfully generated for day_19.pkl \n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31b225ca-b51a-4762-aec6-219d67c8f6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "195    0\n",
       "196    0\n",
       "197    0\n",
       "198    0\n",
       "199    0\n",
       "Length: 200, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5f76b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing day_19.pkl\n",
      "Successfully loaded pkl file. Processing as dataframe.\n",
      "Successfully cleaned DataFrame for processing. Sending to generate returns\n",
      "Successfully generated trading signal. Sending to generate returns\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ask_px_00'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ask_px_00'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#theoretical- hope this works lmao\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result_nvda \u001b[38;5;241m=\u001b[39m signal_and_trade(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNVDA\u001b[39m\u001b[38;5;124m'\u001b[39m, directory)\n\u001b[1;32m      3\u001b[0m result_anf \u001b[38;5;241m=\u001b[39m signal_and_trade(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANF\u001b[39m\u001b[38;5;124m'\u001b[39m, directory)\n",
      "Cell \u001b[0;32mIn[32], line 36\u001b[0m, in \u001b[0;36msignal_and_trade\u001b[0;34m(ticker, directory)\u001b[0m\n\u001b[1;32m     32\u001b[0m signal\u001b[38;5;241m=\u001b[39m moving_average_signal(tkr_df, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSuccessfully generated trading signal. Sending to generate returns\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m returns \u001b[38;5;241m=\u001b[39m execute_trading_signal(tkr_df, signal)\n\u001b[1;32m     37\u001b[0m returns_cumulative \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([returns_cumulative, returns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_return\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturns sucessfully generated for \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m filename)\n",
      "Cell \u001b[0;32mIn[27], line 33\u001b[0m, in \u001b[0;36mexecute_trading_signal\u001b[0;34m(bbo_df, signal, order_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Buy up to 3 levels of the ask side until $1M is spent\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m     ask_price \u001b[38;5;241m=\u001b[39m asks\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mask_px_0\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     34\u001b[0m     ask_size \u001b[38;5;241m=\u001b[39m asks\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mask_sz_0\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m     level_investment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(order_size \u001b[38;5;241m-\u001b[39m investment, ask_price \u001b[38;5;241m*\u001b[39m ask_size)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1146\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m-> 1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4005\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   4002\u001b[0m     series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ixs(col, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   4003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[index]\n\u001b[0;32m-> 4005\u001b[0m series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_item_cache(col)\n\u001b[1;32m   4006\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[1;32m   4008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[1;32m   4009\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[1;32m   4010\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[1;32m   4011\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4414\u001b[0m, in \u001b[0;36mDataFrame._get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   4409\u001b[0m res \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(item)\n\u001b[1;32m   4410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4411\u001b[0m     \u001b[38;5;66;03m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[1;32m   4412\u001b[0m     \u001b[38;5;66;03m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[0;32m-> 4414\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(item)\n\u001b[1;32m   4415\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ixs(loc, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   4417\u001b[0m     cache[item] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ask_px_00'"
     ]
    }
   ],
   "source": [
    "result_nvda = signal_and_trade('NVDA', directory)\n",
    "result_anf = signal_and_trade('ANF', directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061d077-ee58-4bd3-b3f5-2a42817c4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal = moving_average_signal(test, short_window=50, long_window=500, b=0.0001)\n",
    "\n",
    "# returns = execute_trading_signal(test, signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08affd-39d8-451b-9bb2-491550c00790",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns['cumulative_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ced212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# theoretical, hopefully this works\n",
    "\n",
    "returns = []\n",
    "for df in bbo_dfs_anf:\n",
    "    ret = execute_trading_signal(df, signal)\n",
    "    returns.append(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31047294-4819-4760-9e18-ab33fc25f177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mfe230e",
   "language": "python",
   "name": "mfe230e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
